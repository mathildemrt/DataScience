{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Gutenberg Books Corpus - part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the Gutenberg Corpus in the same form as last week. \n",
    "\n",
    "In the [first analysis notebook](https://github.com/dslab2018/dslab2018.github.io/blob/master/notebooks/DSLab_week7_gutenberg_corpus.ipynb) we explored various RDD methods and in the end built an N-gram viewer for the gutenberg books project. Now, we will use the corpus to train a simple language classification model using [Spark's machine learning library](http://spark.apache.org/docs/latest/mllib-guide.html) and Spark DataFrames.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<h3>The structure of this lab is as follows:</h3>\n",
    "\n",
    "<ol>\n",
    "    <li>initializing Spark and loading data</li>\n",
    "    <li>construction of Spark DataFrames</li>\n",
    "    <li>using core DataFrame functionality and comparisons to RDD methods</li>\n",
    "    <li>using the Spark ML library for vectorization</li>\n",
    "    <li>building a classifier pipeline</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and launch the Spark runtime\n",
    "\n",
    "Remember from the previous notebook that we have a saved configuration in `./spark_config/` -- so all we need to do is set the `SPARK_CONF_DIR` environment variable and our default configuration will be used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set this to the base spark directory on your system\n",
    "spark_home = '/users/mathilde/spark-2.2.0-bin-hadoop2.7'\n",
    "try:\n",
    "    import findspark\n",
    "    findspark.init(spark_home)\n",
    "except ModuleNotFoundError as e:\n",
    "    print('Info: {}'.format(e))\n",
    "\n",
    "import getpass\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"Gutenberg text modelling\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Gutenberg text modelling</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11234a7b8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "**TODO**: \n",
    "* download the gutenberg_cleaned_rdd from [here](https://polybox.ethz.ch/index.php/s/rv4VTSmXvJhvq9B) and extract it into the `data` directory in the base path of this repository.\n",
    "* load this as `cleaned_rdd` using `sc.sequenceFile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rdd = sc.sequenceFile( '../data/gutenberg_cleaned_rdd' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.1 ms, sys: 13.6 ms, total: 51.7 ms\n",
      "Wall time: 5.48 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25198"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time cleaned_rdd.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'et_dunlap_publishers_made_in_the_united_states_of_america_copyright_1909_by_w_j_watt_company_with_love_and_gratitude_i_dedicate_this_book_to_my_father_tess_of_the_storm_country_chapter_i_one_september'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_rdd.first()[1][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there were a few further pre-processing steps: we removed all punctuation, made the text lowercase, and replaced whitespace characters with \"_\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the metadata dictionary and broadcast it\n",
    "\n",
    "Just as in the previous notebook, we will load our pre-generated metadata dictionary and broadcast it to all the executors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/gutenberg_metadata.json', 'r') as f :\n",
    "    meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: create meta_b by broadcasting meta_dict\n",
    "meta_b = sc.broadcast(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "A [`DataFrame`](http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes) is analogous to Pandas or R dataframes. They are since v2.0 the \"official\" API for Spark and importantly, the development of the [machine learning library](http://spark.apache.org/docs/latest/ml-guide.html) is focused exclusively on the DataFrame API. Many low-level optimizations have been developed for DataFrames in recent versions of Spark, so that the overheads of using Python with Spark have also been minimized somewhat. Using DataFrames allows you to specify types for your operations which means that they can be offloaded to the Scala backend and optimized by the runtime. \n",
    "\n",
    "However, you frequently will find that there simply is no easy way of doing a particular operation with the DataFrame methods and will need to resort to the lower-level RDD API. \n",
    "\n",
    "## Creating a DataFrame\n",
    "\n",
    "Here we will create a DataFrame out of the RDD that we were using in the previous excercies. The DataFrame is a much more natural fit for this dataset. The inclusion of the book metadata is much more natural here, simply as columns which can then be used in queries. \n",
    "\n",
    "To begin, we will map the RDD elements to type [Row](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row) and recast the data as a DataFrame. Note that we are lazy here and are just using the default `StringType` for all columns, but we could be more specific and use e.g. `IntegerType` for the `gid` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, StructField, StructType\n",
    "\n",
    "# set up the Row \n",
    "df = spark.createDataFrame(\n",
    "    cleaned_rdd.map(lambda x: Row(**meta_b.value[x[0]], text=x[1])), \n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inspection, the `Row` class can be conveniently cast into a `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author_id': '7802',\n",
       " 'author_name': ['White', ' Grace Miller'],\n",
       " 'birth_year': '1873',\n",
       " 'death_year': '1952',\n",
       " 'downloads': '29',\n",
       " 'first_name': 'Grace Miller',\n",
       " 'gid': '22064',\n",
       " 'language': 'en',\n",
       " 'last_name': 'White',\n",
       " 'license': 'Public domain in the USA.',\n",
       " 'subtitle': '',\n",
       " 'text': 'et_dunlap_publishers_made_in_the_united_states_of_america_copyright_1909_by_w_j_watt_company_with_love_and_gratitude_i_dedicate_this_book_to_my_father_tess_of_the_storm_country_chapter_i_one_september_afternoon_not_many_years_ago_three_men_sat_on_the_banks_of_cayuga_lake_cleaning_the_fish_they_had_caught_in_their_nets_the_previous_night_when_they_glanced_up_from_their_work_and_looked_beyond_the_southern_borders_of_the_lake_they_could_see_rising_from_the_mantle_of_forestry_the_towers_and_spires_of_cornell_university_in_ithaca_city_an_observer_would_have_noticed_a_sullen_look_of_hatred_pass_unconsciously_over_their_faces_as_their_eyes_lighted_on_the_distant_buildings_for_the_citizens_of_ithaca_were_the_enemies_of_these_squatter_fishermen_and_thought_that_their_presence_on_the_outskirts_of_the_town_besmirched_its_fair_fame_not_only_did_the_summer_cottages_of_the_townfolk_that_bordered_the_lake_look_down_disdainfully_upon_their_neighbors_the_humble_shanties_of_the_squatter_fishermen_but_their_owners_did_all_they_could_to_drive_the_fishermen_out_of_the_land_none_of_the_squatters_were_allowed_to_have_the_title_of_the_property_upon_which_their_huts_stood_yet_they_clung_with_deathlike_tenacity_to_their_homes_holding_them_through_the_rights_of_the_squatterlaw_which_conceded_them_the_use_of_the_land_when_once_they_raised_a_hut_upon_it_sterner_and_sterner_the_authorities_of_ithaca_had_made_the_game_laws_until_the_fishermen_to_get_the_food_upon_which_they_lived_dared_only_draw_their_nets_by_night_in_the_winter_whilst_the_summer_residents_were_to_be_found_again_in_the_city_nature_herself_made_harder_the_lot_of_these_squatters_by_sealing_the_lake_with_thick_ice_but_they_faced_the_bitter_cold_and_frozen_surroundings_with_stolid_indifference_a_grim_silence_had_reigned_during_which_the_three_men_had_worked_with_feverish_haste_driven_on_by_the_vicissitudes_of_their_unwholesome_lives_moving_his_crooked_legs_upon_the_hot_sand_and_closing_a_red_lid_over_one_white_blind_eye_ben_letts_spoke_viciously_tess_air_that_cussed_said_he_that_she_keeps_on_saying_fishes_can_feel_when_they_gets_cut_she_air_worse_than_that_too_and_she_do_say_put_in_jake_brewer_grasping_a_large_pickerel_and_thrusting_his_blade_into_its_quivering_body_after_removing_the_scales_that_it_hurts_her_insides_to_see_the_critters_wriggle_under_the_knife_she_air_that_bad_too_ben_letts_scratched_his_head_tentatively_she_aint_had_no_bringin_up_he_resumed_again_plying_the_sharpbladed_knife_to_his_scaly_victims_and_they_do_say_as_how_when_she_air_in_a_tantrum_shell_scratch_her_dads_face_jumpin_on_his_back_like_a_cat_orn_air_a_fool_i_say_so_says_i_too_agreed_brewer_no_wonder_his_shoulders_air_humped_but_you_never_hears_as_much_as_a_grunt_from_him_he_knows_he_aint_never_give_her_no_bringins_up_thats_why_some_folks_has_give_their_kids_bringins_up_interposed_ben_letts_with_a_glance_at_the_third_man_who_was_industriously_cleaning_fish_and_had_not_yet_spoken_and_they_haint_turned_out_no_better_than_tessibel_will_at_this_the_industrious_one_turned_i_spose_ye_be_a_hittin_at_my_poor_myry_ben_he_muttered_i_spose_ye_be_but_godll_some_time_let_me_kill_the_man_and_then_ye_wont_be_hittin_at_her_no_more_cause_there_wont_be_nothin_to_hit_at_it_air_dum_hard_to_keep_a_girl_from_the_wrong_way_love_her_all_ye_will_for_an_instant_ben_letts_dropped_his_head_we_always_wondered_who_he_was_but_more_wonder_has_been_goin_on_why_ye_aint_made_no_offer_to_find_the_fellow_aint_had_no_time_said_the_desperate_cleaner_of_fish_had_to_get_bread_and_beans_to_say_nothin_of_bacon_but_why_didnt_ye_send_the_brat_to_the_workhouse_asked_jake_satisfied_longman_as_he_was_called_shook_his_head_i_was_satisfied_to_let_it_stay_was_all_he_answered_my_old_mammy_says_offered_ben_letts_as_how_yer_son_ezy_asked_tessibel_skinner_to_marry_him_and_as_how_she_slicked_him_in_the_face_with_a_dirty_dishrag_he_slowly_closed_the_scarlet_lids_over_his_crossed_eyes_suspending_the_pickerel_in_his_hand_the_while_tess_aint_had_no_mother_remonstrated_longman_after_a_long_silence_pausing_a_moment_in_his_bloody_work_and_allowing_his_eyes_to_rest_upon_the_magnificent_buildings_of_the_university_rearing_above_the_town_and_myry_says_that_them_what_has_ought_to_be_satisfied_just_then_a_shadow_fell_upon_the_shore_of_the_lake_near_the_fishermen_there_air_tess_now_muttered_letts_and_his_two_companions_eyed_a_figure_clad_in_rags_with_flying_coppercolored_hair_and_bare_dirty_feet_which_dropped_down_beside_longman_without_asking_whether_or_no_cleanin_fish_she_queried_cant_ye_see_growled_ben_course_i_can_she_answered_just_wondered_if_ye_knowed_yerselves_where_be_yer_dad_queried_longman_smiling_as_he_caught_up_two_long_fish_depositing_one_beside_him_where_it_flopped_helplessly_about_upon_the_hot_sand_gone_to_ithacy_replied_tessibel_and_without_change_of_expression_or_color_caught_the_floundering_fish_in_her_dirty_fingers_i_air_a_hittin_the_little_devil_on_the_head_with_a_stone_said_she_and_with_a_pointed_rock_she_expertly_tapped_the_fish_three_times_behind_the_beady_eyes_and_threw_him_down_again_motionless_suppose_seein_the_fish_wrigglin_gives_tessibel_mollygrubs_in_her_belly_grinned_jake_brewer_but_ben_letts_broke_in_how_be_yer_toad_today_tessibel_this_he_said_with_a_malevolent_smile_as_he_took_from_his_pocket_a_huge_hunk_of_tobacco_and_munched_a_generous_mouthful_therefrom_pretty_well_answered_tess_pertly_and_measuring_the_blue_water_with_her_eye_she_sent_a_flat_stone_skipping_across_it_then_with_darkening_face_she_wheeled_about_upon_the_heavy_squatter_but_air_it_any_of_yer_business_how_my_toad_air_ben_letts_naw_laughed_ben_nudging_jake_in_the_ribs_with_his_bare_elbow_only_i_thought_as_how_he_might_be_dead_then_he_whispered_to_brewer_wait_till_i_get_at_him_deaddead_who_said_as_how_he_air_dead_ye_int_been_a_rubberin_in_his_hole_have_ye_ben_letts_ben_only_laughed_in_reply_ye_have_ben_letts_ye_have_damn_ye_screamed_the_girl_now_glowering_above_the_fishermen_with_eyes_changing_to_the_deep_copper_of_her_hair_take_that_and_that_and_that_she_had_snatched_the_long_fish_from_his_fingers_and_with_swift_swirls_slapped_it_thrice_into_the_fishermans_face_turning_she_flashed_away_her_long_shadows_giving_out_the_smaller_ones_of_the_tatters_that_hung_about_her_ill_be_goldarned_gasped_letts_and_ill_be_goldarned_twice_if_i_dont_get_even_with_her_some_of_these_here_days_the_devils_built_his_nest_in_her_alright_and_if_hell_fire_dont_get_her_itll_be_cause_she_air_burned_up_by_her_own_cussed_wickedness_he_rubbed_his_face_frantically_with_the_soiled_sleeve_of_his_shirt_spitting_out_the_scales_and_blood_that_hat_lodged_between_his_darkcolored_teeth_yere_always_a_tormentin_her_ben_said_longman_now_if_ye_was_only_satisfied_to_let_her_alone_i_air_a_thinkin_that_she_wouldnt_bother_ye_tess_air_a_good_girl_for_myry_says_as_how_she_can_hush_the_brat_when_he_air_a_howlin_like_a_nigger_shell_cast_a_spell_over_him_thats_what_she_will_muttered_ben_letts_her_ma_could_take_off_warts_afore_she_was_knee_high_to_a_grasshopper_and_so_can_tess_once_she_whispered_ten_off_from_minister_graves_hand_under_his_very_eyes_when_he_was_a_laughin_at_the_idee_wish_theyd_lit_on_his_nose_broke_out_jake_brewer_darkly_he_wouldnt_be_makin_it_so_hard_for_us_down_here_he_gets_his_bread_on_sunday_if_any_man_does_but_they_do_say_as_how_when_he_sees_tess_a_comin_along_he_scoots_like_a_jackrabbit_sposin_the_dominie_dont_laugh_now_sposin_he_dont_put_in_longman_with_a_chuckle_he_air_lost_the_ten_warts_aint_he_tess_aint_the_worst_in_this_here_county_she_can_keep_the_breadrisin_from_comin_up_objected_brewer_she_did_it_with_us_one_day_last_winter_she_scooted_by_our_hut_and_down_dropped_the_yeast_wouldnt_as_much_as_let_her_step_her_foot_in_my_kitchen_bakin_day_air_we_goin_out_again_tonight_fellers_yep_answered_ben_letts_sposin_ornll_go_too_he_air_in_town_but_hell_get_back_orn_will_there_aint_no_man_on_the_shores_of_this_here_lake_that_can_pull_a_net_with_a_steady_hand_like_orn_skinner_pity_he_has_such_a_gal_letts_gave_another_wipe_at_the_scales_which_still_clung_to_his_neck_and_his_eyes_glittered_evilly_as_he_looked_in_the_direction_the_girl_had_taken_he_turned_when_longman_touched_his_arm_for_years_it_had_been_the_custom_of_the_fishermen_to_allow_the_subject_of_netting_to_remain_undiscussed_they_plied_their_trade_spent_a_term_in_prison_if_detected_and_returned_to_again_take_up_their_occupation_of_catching_and_selling_fish_ben_letts_knew_he_was_venturing_upon_dangerous_ground_broad_daylight_he_growled_catching_the_expression_upon_his_companions_face_and_there_aint_no_one_in_sight_thatll_tell_better_be_satisfied_to_keep_yer_mouth_shut_ben_letts_cautioned_longman_nettin_air_bad_for_the_man_what_gets_caught_got_any_bait_out_there_he_finished_pointing_lakeward_to_a_bobbing_box_anchored_a_distance_from_the_shore_not_a_damn_bit_replied_jake_brewer_dont_need_it_now_keep_the_bait_cars_a_floatin_to_blind_the_eyes_of_some_guy_that_might_be_a_rubberin_they_dont_know_a_minnie_from_a_whale_those_city_coves_dont_aint_that_orns_boat_comin_under_the_shadders',\n",
       " 'title': 'Tess of the Storm Country'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first row\n",
    "df.first().asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author_id',\n",
       " 'author_name',\n",
       " 'birth_year',\n",
       " 'death_year',\n",
       " 'downloads',\n",
       " 'first_name',\n",
       " 'gid',\n",
       " 'language',\n",
       " 'last_name',\n",
       " 'license',\n",
       " 'subtitle',\n",
       " 'text',\n",
       " 'title']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame includes convenience methods for quickly inspecting the data. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        birth_year|\n",
      "+-------+------------------+\n",
      "|  count|             20934|\n",
      "|   mean|1829.9672587614018|\n",
      "| stddev|114.48079532175812|\n",
      "|    min|           -100 BC|\n",
      "|    max|               973|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('birth_year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain operations are much more covenient with the DataFrame API, such as `groupBy`, which yields a special [`GroupedData`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) object. Check out the API for the different operations you can perform on grouped data -- here we will use `count` to get the equivalent of our author-count from the previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         author_name|count|\n",
      "+--------------------+-----+\n",
      "|           [Various]| 1654|\n",
      "|                null|  835|\n",
      "|         [Anonymous]|  278|\n",
      "|[Balzac,  Honoré de]|  121|\n",
      "|[Kingston,  Willi...|  113|\n",
      "|      [Twain,  Mark]|  104|\n",
      "|[Ballantyne,  R. ...|   95|\n",
      "|[Jacobs,  W. W. (...|   94|\n",
      "|           [Unknown]|   92|\n",
      "|[Shakespeare,  Wi...|   87|\n",
      "|    [Pepys,  Samuel]|   85|\n",
      "|[Fenn,  George Ma...|   83|\n",
      "| [Dumas,  Alexandre]|   75|\n",
      "|     [Verne,  Jules]|   74|\n",
      "|     [Sand,  George]|   73|\n",
      "|[Howells,  Willia...|   70|\n",
      "|[Churchill,  Wins...|   67|\n",
      "| [Dickens,  Charles]|   61|\n",
      "|[Henty,  G. A. (G...|   60|\n",
      "|[Doyle,  Arthur C...|   58|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: complete the cell to use groupBy to show the most common authors\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "(df.groupby( \"author_name\" )\n",
    "   .count()\n",
    "   .sort(desc(\"count\"))\n",
    "   .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing columns\n",
    "\n",
    "Columns can be accessed in a variety of ways, and usually you can just pass a column name to DataFrame methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|birth_year|\n",
      "+----------+\n",
      "|      1873|\n",
      "|      1871|\n",
      "|      1870|\n",
      "|      1842|\n",
      "|      1774|\n",
      "|      1850|\n",
      "|      1802|\n",
      "|      1802|\n",
      "|      null|\n",
      "|      null|\n",
      "|      1925|\n",
      "|      1863|\n",
      "|      1843|\n",
      "|      1873|\n",
      "|      1886|\n",
      "|      1886|\n",
      "|      1856|\n",
      "|      1791|\n",
      "|      1858|\n",
      "|      null|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('birth_year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, columns are also objects, so they have methods of their own that can be useful. See [here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=isin#pyspark.sql.Column). You can access them very simply like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'birth_year'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.birth_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new columns\n",
    "\n",
    "Lets make a new column with a publication date similar to the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('publication_year', (df.birth_year + 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Show author name, title and publication year; sort by publication_year in descending order of publication year\n",
    "\n",
    "Hint: you can pass column names, column instances (e.g. `df.birth_year`) or an SQL query (for this you need to register a table first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------------+\n",
      "|      author_name|               title|publication_year|\n",
      "+-----------------+--------------------+----------------+\n",
      "|    [Blade,  Zoë]|            Identity|          2021.0|\n",
      "|    [Blade,  Zoë]|     Less than Human|          2021.0|\n",
      "|[Doctorow,  Cory]|Super Man and the...|          2011.0|\n",
      "|[Doctorow,  Cory]|          Printcrime|          2011.0|\n",
      "|[Doctorow,  Cory]|           Craphound|          2011.0|\n",
      "|[Doctorow,  Cory]|Return to Pleasur...|          2011.0|\n",
      "|[Doctorow,  Cory]|Shadow of the Mot...|          2011.0|\n",
      "|[Doctorow,  Cory]|  A Place so Foreign|          2011.0|\n",
      "|[Doctorow,  Cory]|      Little Brother|          2011.0|\n",
      "|[Doctorow,  Cory]|Someone Comes to ...|          2011.0|\n",
      "|[Doctorow,  Cory]|Home Again, Home ...|          2011.0|\n",
      "|[Doctorow,  Cory]|Eastern Standard ...|          2011.0|\n",
      "|[Doctorow,  Cory]|Ebooks: Neither E...|          2011.0|\n",
      "|[Camacho,  Jorge]|La Majstro kaj Ma...|          2006.0|\n",
      "|[Camacho,  Jorge]|La liturgio de l'...|          2006.0|\n",
      "|[Camacho,  Jorge]|La liturgio de l'...|          2006.0|\n",
      "|[Vaknin,  Samuel]|The Suffering of ...|          2001.0|\n",
      "|[Vaknin,  Samuel]|   The Capgras Shift|          2001.0|\n",
      "|[Vaknin,  Samuel]|Cyclopedia of Phi...|          2001.0|\n",
      "| [Obama,  Barack]|Inaugural Preside...|          2001.0|\n",
      "+-----------------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.author_name,df.title,df.publication_year).sort(df.publication_year.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language classification with Spark ML\n",
    "\n",
    "Here we will use some of the same techniques we developed in the last excercise, but this time we will use the built-in methods of the [Spark ML library](http://spark.apache.org/docs/2.2.0/api/python/pyspark.ml#) instead of coding up our own transformation functions. We will apply the N-Gram technique to build a simple language classification model. \n",
    "\n",
    "The method is rather straightforward and outlined in [Cavnar & Trenkle 1994](http://odur.let.rug.nl/~vannoord/TextCat/textcat.pdf):\n",
    "\n",
    "For each of the English/German training sets:\n",
    "\n",
    "1. tokenize the text (spaces are also tokens, so we replace them with \"_\")\n",
    "2. extract N-grams where 1 < N < 5\n",
    "3. determine the most common N-grams for each corpus\n",
    "4. encode both sets of documents using the combined top ngrams\n",
    "\n",
    "\n",
    "## Character tokens vs. Word tokens\n",
    "In the last notebook, we used words as \"tokens\" -- now we will use characters, even accounting for white space (which we have replaced with \"_\" above). We will use the two example sentences again:\n",
    "\n",
    "    document 1: \"a dog bit me\"\n",
    "    document 2: \"i bit the dog back\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkML feature transformers\n",
    "\n",
    "The SparkML library includes many data transformers that all support the same API (much in the same vein as Scikit-Learn). Here we are using the [`CountVectorizer`](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.CountVectorizer), [`NGram`](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.NGram) and [`RegexTokenizer`](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RegexTokenizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, NGram, RegexTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the transformations\n",
    "\n",
    "We instantiate the three transformers that will be applied in turn. We will pass the output of one as the input of the next -- in the end our DataFrame will contain a column `vectors` that will be the vectorized version of the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", gaps=False, pattern='\\S')\n",
    "ngram = NGram(n=2, inputCol='tokens', outputCol='ngrams')\n",
    "count_vectorizer = CountVectorizer(inputCol=\"ngrams\", outputCol=\"vectors\", vocabSize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets see what this does to our test sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text='a dog bit me'), Row(text='i bit the dog back')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = spark.createDataFrame([('a dog bit me',), ('i bit the dog back',)], ['text'])\n",
    "\n",
    "test_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Figure out how to run the `test_df` through the two transformers and generate an `test_ngram_df`. `show()` the `text`, `tokens`, and `ngrams` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+\n",
      "|              text|              tokens|              ngrams|\n",
      "+------------------+--------------------+--------------------+\n",
      "|      a dog bit me|[a, d, o, g, b, i...|[a d, d o, o g, g...|\n",
      "|i bit the dog back|[i, b, i, t, t, h...|[i b, b i, i t, t...|\n",
      "+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_ngram_df = ngram.transform( regex_tokenizer.transform( test_df ) )\n",
    "test_ngram_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Fit the `CountVectorizer` with `n=2` ngrams and store in `test_cv_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_cv_model = count_vectorizer.fit(test_ngram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d o',\n",
       " 'g b',\n",
       " 'b i',\n",
       " 'o g',\n",
       " 'i t',\n",
       " 'a d',\n",
       " 'b a',\n",
       " 't m',\n",
       " 'i b',\n",
       " 'h e',\n",
       " 'a c',\n",
       " 'e d',\n",
       " 't t',\n",
       " 'm e',\n",
       " 't h',\n",
       " 'c k']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the constructed vocabulary\n",
    "test_cv_model.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: transform `test_ngram_df` into vectors and `show` them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|              text|              tokens|              ngrams|             vectors|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|      a dog bit me|[a, d, o, g, b, i...|[a d, d o, o g, g...|(16,[0,1,2,3,4,5,...|\n",
      "|i bit the dog back|[i, b, i, t, t, h...|[i b, b i, i t, t...|(16,[0,1,2,3,4,6,...|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cv_model.transform( test_ngram_df ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipelines\n",
    "\n",
    "Keeping track of these steps is a bit tedious -- if we wanted to repeat the above steps on different data, we would either have to write a wrapper function or re-execute all the cells again. It would be great if we could create a *pipeline* that encapsulated these steps and all we had to do was provide the inputs and parameters. \n",
    "\n",
    "The Spark ML library includes this concept of [Pipelines](https://spark.apache.org/docs/2.2.0/ml-pipeline.html) and we can use it to simplify complex ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** define a `list` of pipeline stages that consists of the tokenizers and the `CountVectorizer` we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        regex_tokenizer,\n",
    "        ngram,\n",
    "        count_vectorizer\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|              text|              tokens|              ngrams|             vectors|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|      a dog bit me|[a, d, o, g, b, i...|[a d, d o, o g, g...|(16,[0,1,2,3,4,5,...|\n",
      "|i bit the dog back|[i, b, i, t, t, h...|[i b, b i, i t, t...|(16,[0,1,2,3,4,6,...|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Executing the pipeline\n",
    "(\n",
    "    cv_pipeline.fit(test_df)\n",
    "               .transform(test_df)\n",
    "               .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more concise and much less error prone! The really cool thing about pipelines is that I can now very easily change the parameters of the different components. Imagine we wanted to fit trigrams (`n=3`) instead of bigrams (`n=2`), and we wanted to change the name of the final column. We can reuse the same pipeline but feed it a *parameter map* specifying the changed parameter value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|              text|              tokens|              ngrams|             vectors|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|      a dog bit me|[a, d, o, g, b, i...|[a d o, d o g, o ...|(16,[0,1,2,9,11,1...|\n",
      "|i bit the dog back|[i, b, i, t, t, h...|[i b i, b i t, i ...|(16,[0,1,2,3,4,5,...|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note the dictionaries added to fit() and transform() arguments\n",
    "(\n",
    "    cv_pipeline.fit(test_df, {ngram.n:3})\n",
    "               .transform(test_df, {count_vectorizer.outputCol: 'new_vectors'})\n",
    "               .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a more complex pipeline\n",
    "\n",
    "For our language classification we want to use ngrams 1-3. We can build a function that will yield a pipeline with this more complex setup. Our procedure here is like this:\n",
    "\n",
    "1. tokenize as before\n",
    "2. assemble the ngram transformers to yield n=1, n=2, etc columns\n",
    "3. vectorize using each set of ngrams giving partial vectors\n",
    "4. assemble the vectors into one complete feature vector using [VectorAssembler](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def ngram_vectorize(min_n=1, max_n=1, min_df=1):\n",
    "    \"\"\"Use a range of ngrams to vectorize a corpus\"\"\"\n",
    "    \n",
    "    tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", gaps=False, pattern='\\S')\n",
    "    ngrams = []    \n",
    "    count_vectorizers = []\n",
    "    \n",
    "     \n",
    "    for i in range(min_n, max_n+1):\n",
    "        ngrams.append(\n",
    "            NGram(n=i, inputCol='tokens', outputCol='ngrams_'+str(i))\n",
    "        )\n",
    "        count_vectorizers.append(\n",
    "            CountVectorizer(inputCol='ngrams_'+str(i), outputCol='vectors_'+str(i), vocabSize=1000, minDF=min_df)\n",
    "        )\n",
    "    \n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=['vectors_'+str(i) for i in range(min_n, max_n+1)], outputCol='features')\n",
    "    \n",
    "    return Pipeline(stages=[tokenizer] + ngrams + count_vectorizers + [assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                         |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(44,[0,1,2,3,4,5,6,7,11,12,13,14,15,16,17,19,25,28,29,30,37,39,40,41],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |\n",
      "|[2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0]|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorize(1,3).fit(test_df).transform(test_df).select('features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the DataFrames and models\n",
    "\n",
    "For our language classifier we will use just two languages (English and either German or French). We need to create a DataFrame that is filtered to just include those languages. \n",
    "\n",
    "In addition, we will need this step of transforming raw string documents into vectors when we try the classifier on new data. We should therefore save the fitted NGram model for later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** use the [`isin`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=isin#pyspark.sql.Column.isin) method of the `language` column to filter the DF down to \"en\", \"de\" and \"fr\" languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lang_df = df.filter( df.language.isin('en','de','fr') ).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Construct the `ngram_model` by using `ngram_vectorize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngram_model = ngram_vectorize().fit(lang_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(700, {0: 1720.0, 1: 973.0, 2: 690.0, 3: 522.0, 4: 471.0, 5: 518.0, 6: 520.0, 7: 500.0, 8: 402.0, 9: 541.0, 10: 284.0, 11: 329.0, 12: 187.0, 13: 156.0, 14: 136.0, 15: 142.0, 16: 164.0, 17: 175.0, 18: 109.0, 19: 153.0, 20: 145.0, 21: 48.0, 22: 82.0, 23: 4.0, 24: 13.0, 25: 1.0, 26: 9.0, 28: 3.0, 32: 1.0, 33: 2.0}))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model.transform(lang_df).select('features').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier\n",
    "\n",
    "We have successfully transformed the dataset into a representation that we can (almost) feed into a classifier. What we need still is a label column as well the final stage of the pipeline that will fit the actual model. \n",
    "\n",
    "To generate labels from the language column, we will use the `StringIndexer` as a part of our pipeline. For the classification we will use the simplest possible `LogisticRegression` -- once you've convinced yourself that you know how it works, go ahead and experiment with other [classifiers](http://spark.apache.org/docs/latest/api/python/pyspark.ml#module-pyspark.ml.classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Set up a `classification_pipeline`. Use the N-gram model we defined above as a starting stage, followed by a `StringIndexer` and a `LogisticRegression` classifier. Make sure you read the documentation on these!\n",
    "\n",
    "Note that we can use the pre-trained N-gram model -- the `Pipeline` will automatically infer that the stage is already complete and will only use it in the transformation step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classification_pipeline = Pipeline(\n",
    "    stages=[ngram_model, \n",
    "            StringIndexer(inputCol='language', outputCol='label'),\n",
    "            LogisticRegression(regParam=0.002, elasticNetParam=1, maxIter=10)\n",
    "           ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the classifier! The fitting will take a while -- you may want to run this first on a subset of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Use the `randomSplit` DataFrame method to generate `training` and `test` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the training and test sets\n",
    "training, test = lang_df.randomSplit([1.0, 2.0], 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 341 ms, sys: 138 ms, total: 479 ms\n",
      "Wall time: 60 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "classifier = classification_pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for en\n",
      "+-----+----------------------------------------------------------------+----------+\n",
      "|label|probability                                                     |prediction|\n",
      "+-----+----------------------------------------------------------------+----------+\n",
      "|0.0  |[0.9979993781099208,0.001039362152528426,9.612597375505918E-4]  |0.0       |\n",
      "|0.0  |[0.9976538228914836,0.0015818677645751845,7.643093439411799E-4] |0.0       |\n",
      "|0.0  |[0.9979762175896993,0.0011322976565881265,8.914847537125246E-4] |0.0       |\n",
      "|0.0  |[0.996407261995345,0.0021970487479567513,0.0013956892566981687] |0.0       |\n",
      "|0.0  |[0.9986060039348046,8.060316509569959E-4,5.879644142384917E-4]  |0.0       |\n",
      "|0.0  |[0.997611633941847,0.001537629594641789,8.507364635112391E-4]   |0.0       |\n",
      "|0.0  |[0.996996826984687,0.002120659447283608,8.825135680295064E-4]   |0.0       |\n",
      "|0.0  |[0.9979594567837877,8.954057733882497E-4,0.0011451374428241771] |0.0       |\n",
      "|0.0  |[0.9958908037837282,0.0028649234477379553,0.0012442727685339529]|0.0       |\n",
      "|0.0  |[0.996074425006601,0.0025067324001233067,0.0014188425932755487] |0.0       |\n",
      "+-----+----------------------------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Predictions for fr\n",
      "+-----+----------------------------------------------------------------+----------+\n",
      "|label|probability                                                     |prediction|\n",
      "+-----+----------------------------------------------------------------+----------+\n",
      "|1.0  |[0.0026342291691377594,0.9959200250459109,0.0014457457849512672]|1.0       |\n",
      "|1.0  |[0.002995704245576172,0.9954589626054705,0.001545333148953287]  |1.0       |\n",
      "|1.0  |[0.004766288672496285,0.9932931865779471,0.0019405247495565142] |1.0       |\n",
      "|1.0  |[0.005678570033222564,0.9914813240402376,0.0028401059265398304] |1.0       |\n",
      "|1.0  |[8.04362046875612E-4,0.9985749932113867,6.206447417377328E-4]   |1.0       |\n",
      "|1.0  |[0.0031267620178440625,0.995041885735673,0.0018313522464831668] |1.0       |\n",
      "|1.0  |[0.0055239755417463,0.9923132616549962,0.0021627628032572853]   |1.0       |\n",
      "|1.0  |[0.0027769327963601824,0.9955139263220139,0.0017091408816260758]|1.0       |\n",
      "|1.0  |[0.0070023018935332654,0.9905286037891355,0.0024690943173313424]|1.0       |\n",
      "|1.0  |[0.003599456402579537,0.9948487734665218,0.0015517701308986353] |1.0       |\n",
      "+-----+----------------------------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Predictions for de\n",
      "+-----+--------------------------------------------------------------+----------+\n",
      "|label|probability                                                   |prediction|\n",
      "+-----+--------------------------------------------------------------+----------+\n",
      "|2.0  |[0.012814396342430204,0.011285758636708122,0.9758998450208616]|2.0       |\n",
      "|2.0  |[0.004183429401479204,0.007937728154901999,0.9878788424436188]|2.0       |\n",
      "|2.0  |[0.004170521272369776,0.007929116815616824,0.9879003619120134]|2.0       |\n",
      "|2.0  |[5.114140260350687E-4,0.002320736960916737,0.9971678490130483]|2.0       |\n",
      "|2.0  |[7.391072165099053E-4,0.004009307535838944,0.9952515852476511]|2.0       |\n",
      "|2.0  |[7.354064299834967E-4,0.003991695965804216,0.9952728976042122]|2.0       |\n",
      "|2.0  |[0.002125867632916342,0.0076945072356076885,0.990179625131476]|2.0       |\n",
      "|2.0  |[0.00496741279528139,0.007074069749613886,0.9879585174551047] |2.0       |\n",
      "|2.0  |[0.02378947219422494,0.015074837777558343,0.9611356900282167] |2.0       |\n",
      "|2.0  |[0.005657111778850162,0.0082948376292485,0.9860480505919014]  |2.0       |\n",
      "+-----+--------------------------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the predictions \n",
    "for lang in ['en', 'fr', 'de']:\n",
    "    print('Predictions for {0}'.format(lang))\n",
    "    (classifier.transform(\n",
    "        test.filter(test.language == lang))\n",
    "            .select('label', 'probability', 'prediction')\n",
    "            .show(10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be seeing mostly good agreement between `label` and `prediction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the model and continuing the exploration of the data\n",
    "\n",
    "We have completed the basic model training, but many improvements are possible. One obvious improvement is hyperparameter tuning -- check out the [docs](http://spark.apache.org/docs/latest/ml-tuning.html#ml-tuning-model-selection-and-hyperparameter-tuning) for some examples and try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some other ideas for things you could do with this dataset: \n",
    "\n",
    "* try other [classifiers that are included in MLlib](http://spark.apache.org/docs/latest/mllib-classification-regression.html)\n",
    "* build a regression model to predict year of publication (may be better with word ngrams)\n",
    "* do clustering on the english books and see if sub-groups of the language pop up\n",
    "* cluster by author -- do certain authors write in similar ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
